proxy:
  secretToken:
  annotations: {}
  deploymentStrategy:
    ## type: Recreate
    ## - JupyterHub's interaction with the CHP proxy becomes a lot more robust
    ##   with this configuration. To understand this, consider that JupyterHub
    ##   during startup will interact a lot with the k8s service to reach a
    ##   ready proxy pod. If the hub pod during a helm upgrade is restarting
    ##   directly while the proxy pod is making a rolling upgrade, the hub pod
    ##   could end up running a sequence of interactions with the old proxy pod
    ##   and finishing up the sequence of interactions with the new proxy pod.
    ##   As CHP proxy pods carry individual state this is very error prone. One
    ##   outcome when not using Recreate as a strategy has been that user pods
    ##   have been deleted by the hub pod because it considered them unreachable
    ##   as it only configured the old proxy pod but not the new before trying
    ##   to reach them.
    type: Recreate
    ## rollingUpdate:
    ## - WARNING:
    ##   This is required to be set explicitly blank! Without it being
    ##   explicitly blank, k8s will let eventual old values under rollingUpdate
    ##   remain and then the Deployment becomes invalid and a helm upgrade would
    ##   fail with an error like this:
    ##
    ##     UPGRADE FAILED
    ##     Error: Deployment.apps "proxy" is invalid: spec.strategy.rollingUpdate: Forbidden: may not be specified when strategy `type` is 'Recreate'
    ##     Error: UPGRADE FAILED: Deployment.apps "proxy" is invalid: spec.strategy.rollingUpdate: Forbidden: may not be specified when strategy `type` is 'Recreate'
    rollingUpdate:
  # service relates to the proxy-public service
  service:
    type: LoadBalancer
    labels: {}
    annotations:
      cloud.google.com/load-balancer-type: "Internal"
    nodePorts:
      http:
      https:
    disableHttpPort: false
    extraPorts: []
    loadBalancerIP: 10.148.15.241
    loadBalancerSourceRanges: []
  chp:
    nodeSelector:
      cloud.google.com/gke-nodepool: jupyter

scheduling:
  userScheduler:
    enabled: false

singleuser:
  nodeSelector:
    cloud.google.com/gke-nodepool: jupyter
  memory:
    guarantee: 0.1G
    limit: 12G
  extraEnv:
    GRANT_SUDO: "yes"
    NOTEBOOK_ARGS: "--allow-root"
  cloudMetadata:
    blockWithIptables: false
  networkPolicy:
    enabled: false
  uid: 0
  cmd: start-singleuser.sh
  storage:
    dynamic:
      storageClass: standard
    extraVolumes:
      - name: jupyterhub-shared
        persistentVolumeClaim:
          claimName: nfs
    extraVolumeMounts:
      - name: jupyterhub-shared
        mountPath: /home/jovyan/shared
  image:
    name: favedatascience/notebook
    tag: "3.8.8-spark-flink-pyhive-2022-12-16"

# turns off servers automatically
cull:
  enabled: true
  timeout: 86400
  every: 300

prePuller:
  hook:
    enabled: false
    pullOnlyOnChanges: true

hub:
  extraConfig:
    allowPrivilegeEscalationForSudo: |
      # unbreak sudo, see: https://github.com/jupyterhub/zero-to-jupyterhub-k8s/issues/2429
      c.KubeSpawner.allow_privilege_escalation = True
  nodeSelector:
    cloud.google.com/gke-nodepool: jupyter
  config:
    GoogleOAuthenticator:
      client_id: 111413986291-4p147kljlf0u3a315c669ghe9jrc36k4.apps.googleusercontent.com
      client_secret: hXKgQ7EoQMn1PzMrJWMCBXo0
      oauth_callback_url: https://jupyterhub.myfavedata.com/hub/oauth_callback
      hosted_domain:
        - mesolitica.com
      login_service: myfave
    JupyterHub:
      authenticator_class: google
    Authenticator:
      auto_login: true
      admin_users:
        - "huseinzol05"
# prePuller:
#   hook:
#     enabled: false
#   continuous:
#     enabled: false
